{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognization on Minist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import cv2 #type:ignore\n",
    "import tensorflow as tf #type:ignore\n",
    "from tensorflow.keras import layers, models #type:ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #type:ignore\n",
    "from tensorflow.keras.preprocessing import image #type:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test data have 10 classess  0 to 9 Total classes=10\n",
    "#Approximately 42000 images for training dataset\n",
    "\n",
    "train_dir = '/home/husnain/Desktop/PL/Python/REMOTE_INTERNSHIP/Grow_Intern/3_Handwritten_digit_recognization/training_data'\n",
    "test_dir = '/home/husnain/Desktop/PL/Python/REMOTE_INTERNSHIP/Grow_Intern/3_Handwritten_digit_recognization/test_data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfoam The images pixels values from 0 to 1 range\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)  \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42000 images belonging to 10 classes.\n",
      "Found 41594 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image Resizing 28*28\n",
    "# Batch Size  64\n",
    "# Label Encoding Catagorical on 10 classes\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(28, 28), \n",
    "    batch_size=64,\n",
    "    class_mode='categorical' \n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(28, 28),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test labels: (64, 10)\n",
      "Shape of training labels: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of labels in the Test generator\n",
    "print(\"Shape of test labels:\", test_generator[0][1].shape)\n",
    "# Print the shape of labels in the training generator\n",
    "print(\"Shape of training labels:\", train_generator[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 3)))  \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comiple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "656/656 [==============================] - 81s 122ms/step - loss: 0.2154 - accuracy: 0.9346 - val_loss: 0.0574 - val_accuracy: 0.9818\n",
      "Epoch 2/15\n",
      "656/656 [==============================] - 47s 71ms/step - loss: 0.0587 - accuracy: 0.9817 - val_loss: 0.0343 - val_accuracy: 0.9890\n",
      "Epoch 3/15\n",
      "656/656 [==============================] - 54s 82ms/step - loss: 0.0402 - accuracy: 0.9871 - val_loss: 0.0288 - val_accuracy: 0.9911\n",
      "Epoch 4/15\n",
      "656/656 [==============================] - 48s 72ms/step - loss: 0.0310 - accuracy: 0.9896 - val_loss: 0.0226 - val_accuracy: 0.9927\n",
      "Epoch 5/15\n",
      "656/656 [==============================] - 49s 75ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.0201 - val_accuracy: 0.9931\n",
      "Epoch 6/15\n",
      "656/656 [==============================] - 53s 80ms/step - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.0153 - val_accuracy: 0.9953\n",
      "Epoch 7/15\n",
      "656/656 [==============================] - 50s 76ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0250 - val_accuracy: 0.9917\n",
      "Epoch 8/15\n",
      "656/656 [==============================] - 52s 79ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.0073 - val_accuracy: 0.9980\n",
      "Epoch 9/15\n",
      "656/656 [==============================] - 46s 70ms/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 0.0076 - val_accuracy: 0.9975\n",
      "Epoch 10/15\n",
      "656/656 [==============================] - 39s 60ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
      "Epoch 11/15\n",
      "656/656 [==============================] - 44s 68ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0095 - val_accuracy: 0.9966\n",
      "Epoch 12/15\n",
      "656/656 [==============================] - 45s 69ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0038 - val_accuracy: 0.9986\n",
      "Epoch 13/15\n",
      "656/656 [==============================] - 52s 79ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0060 - val_accuracy: 0.9981\n",
      "Epoch 14/15\n",
      "656/656 [==============================] - 56s 86ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0225 - val_accuracy: 0.9920\n",
      "Epoch 15/15\n",
      "656/656 [==============================] - 51s 78ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0052 - val_accuracy: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcaa4f3db50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train 15 epoches if we cant get high accuranct increase the size of epochs\n",
    "\n",
    "model.fit(\n",
    "         \n",
    "             train_generator,\n",
    "             steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "             epochs=15,\n",
    "             validation_data=test_generator,\n",
    "             validation_steps=test_generator.samples // test_generator.batch_size\n",
    "         )\n",
    "\n",
    "#we got 99% Accurancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93898 (366.79 KB)\n",
      "Trainable params: 93898 (366.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 1: Predicted 5, True 5\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Image NO 2: Predicted 5, True 5\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Image NO 3: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Image NO 4: Predicted 4, True 4\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 5: Predicted 9, True 9\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Image NO 6: Predicted 8, True 8\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 7: Predicted 0, True 0\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Image NO 8: Predicted 9, True 9\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "Image NO 9: Predicted 1, True 1\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "Image NO 10: Predicted 1, True 1\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Image NO 11: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Image NO 12: Predicted 8, True 8\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 13: Predicted 6, True 6\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "Image NO 14: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 15: Predicted 0, True 0\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 16: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 17: Predicted 9, True 9\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 18: Predicted 9, True 9\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 19: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Image NO 20: Predicted 5, True 5\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Image NO 21: Predicted 5, True 5\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Image NO 22: Predicted 9, True 9\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Image NO 23: Predicted 1, True 1\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Image NO 24: Predicted 7, True 7\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Image NO 25: Predicted 3, True 3\n",
      "From 25 images Total Accurate Perdictions are 25\n"
     ]
    }
   ],
   "source": [
    "#count how many times it should be correct \n",
    "count_corrrect_perdiction=0\n",
    "for i in range(25):\n",
    "    test_image, test_label = test_generator[i]\n",
    "    \n",
    "    predictions = model.predict(test_image)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    true_class = np.argmax(test_label, axis=1)\n",
    "    print(f\"Image NO {i + 1}: Predicted {predicted_class[0]}, True {true_class[0]}\")\n",
    "    \n",
    "    if predicted_class[0]==true_class[0]:\n",
    "        count_corrrect_perdiction=count_corrrect_perdiction+1\n",
    "        \n",
    "        \n",
    "print(f\"From 25 images Total Accurate Perdictions are {count_corrrect_perdiction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
